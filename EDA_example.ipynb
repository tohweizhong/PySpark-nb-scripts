{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_path = \"C:/stack/spark-1.6.2-bin-hadoop2.6\"\n",
    "\n",
    "os.environ['SPARK_HOME'] = spark_path\n",
    "os.environ['HADOOP_HOME'] = spark_path\n",
    "\n",
    "sys.path.append(spark_path + \"/bin\")\n",
    "sys.path.append(spark_path + \"/python\")\n",
    "sys.path.append(spark_path + \"/python/pyspark/\")\n",
    "sys.path.append(spark_path + \"/python/lib\")\n",
    "sys.path.append(spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.append(spark_path + \"/python/lib/py4j-0.9-src.zip\")\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = SparkContext(\"local\")\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x19f682a3ba8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc\n",
    "sqlCtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand, randn\n",
    "df = sqlCtx.range(0, 10)\n",
    "df.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+\n",
      "| id|            uniform|             normal|\n",
      "+---+-------------------+-------------------+\n",
      "|  0|0.41371264720975787| 0.5888539012978773|\n",
      "|  1| 0.7311719281896606| 0.8645537008427937|\n",
      "|  2| 0.9031701155118229| 1.2524569684217643|\n",
      "|  3|0.09430205113458567| -2.573636861034734|\n",
      "|  4|0.38340505276222947| 0.5469737451926588|\n",
      "|  5| 0.5569246135523511|0.17431283601478723|\n",
      "|  6| 0.4977441406613893|-0.7040284633147095|\n",
      "|  7| 0.2076666106201438| 0.4637547571868822|\n",
      "|  8| 0.9571919406508957|  0.920722532496133|\n",
      "|  9| 0.7429395461204413|-1.4353459012380192|\n",
      "+---+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\", rand(seed=10).alias(\"uniform\"), randn(seed=27).alias(\"normal\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+\n",
      "| id|            uniform|             normal|\n",
      "+---+-------------------+-------------------+\n",
      "|  0|0.41371264720975787| 0.5888539012978773|\n",
      "|  1| 0.7311719281896606| 0.8645537008427937|\n",
      "|  2| 0.9031701155118229| 1.2524569684217643|\n",
      "|  3|0.09430205113458567| -2.573636861034734|\n",
      "|  4|0.38340505276222947| 0.5469737451926588|\n",
      "|  5| 0.5569246135523511|0.17431283601478723|\n",
      "|  6| 0.4977441406613893|-0.7040284633147095|\n",
      "|  7| 0.2076666106201438| 0.4637547571868822|\n",
      "|  8| 0.9571919406508957|  0.920722532496133|\n",
      "|  9| 0.7429395461204413|-1.4353459012380192|\n",
      "+---+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlCtx.range(0, 10).withColumn('uniform', rand(seed=10)).withColumn('normal', randn(seed=27))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------------+\n",
      "|summary|            uniform|              normal|\n",
      "+-------+-------------------+--------------------+\n",
      "|  count|                 10|                  10|\n",
      "|   mean| 0.5488228646413278|0.009861721586543392|\n",
      "| stddev| 0.2856822245344392|  1.2126061129356596|\n",
      "|    min|0.09430205113458567|  -2.573636861034734|\n",
      "|    max| 0.9571919406508957|  1.2524569684217643|\n",
      "+-------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('uniform', 'normal').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+------------------+\n",
      "|      avg(uniform)|       min(uniform)|      max(uniform)|\n",
      "+------------------+-------------------+------------------+\n",
      "|0.5488228646413278|0.09430205113458567|0.9571919406508957|\n",
      "+------------------+-------------------+------------------+\n",
      "\n",
      "+-------------------+-------------------+-------------------+\n",
      "|            uniform|            uniform|             normal|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|0.41371264720975787|0.41371264720975787| 0.5888539012978773|\n",
      "| 0.7311719281896606| 0.7311719281896606| 0.8645537008427937|\n",
      "| 0.9031701155118229| 0.9031701155118229| 1.2524569684217643|\n",
      "|0.09430205113458567|0.09430205113458567| -2.573636861034734|\n",
      "|0.38340505276222947|0.38340505276222947| 0.5469737451926588|\n",
      "| 0.5569246135523511| 0.5569246135523511|0.17431283601478723|\n",
      "| 0.4977441406613893| 0.4977441406613893|-0.7040284633147095|\n",
      "| 0.2076666106201438| 0.2076666106201438| 0.4637547571868822|\n",
      "| 0.9571919406508957| 0.9571919406508957|  0.920722532496133|\n",
      "| 0.7429395461204413| 0.7429395461204413|-1.4353459012380192|\n",
      "+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, min, max\n",
    "df.select([mean('uniform'), min('uniform'), max('uniform')]).show()\n",
    "df.select([('uniform'), ('uniform'), ('normal')]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "df = sqlCtx.range(0, 10).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00834685407056579"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.cov('rand1', 'rand2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.166666666666666"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.cov('id', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1099396246708271"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.corr('rand1', 'rand2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.corr('id', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "| name|   item|\n",
      "+-----+-------+\n",
      "|Alice|   milk|\n",
      "|  Bob|  bread|\n",
      "| Mike| butter|\n",
      "|Alice| apples|\n",
      "|  Bob|oranges|\n",
      "| Mike|   milk|\n",
      "|Alice|  bread|\n",
      "|  Bob| butter|\n",
      "| Mike| apples|\n",
      "|Alice|oranges|\n",
      "+-----+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+------+-------+------+----+-----+\n",
      "|name_item|apples|oranges|butter|milk|bread|\n",
      "+---------+------+-------+------+----+-----+\n",
      "|      Bob|     6|      7|     7|   6|    7|\n",
      "|     Mike|     7|      6|     7|   7|    6|\n",
      "|    Alice|     7|      7|     6|   7|    7|\n",
      "+---------+------+-------+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with two columns (name, item)\n",
    "names = [\"Alice\", \"Bob\", \"Mike\"]\n",
    "items = [\"milk\", \"bread\", \"butter\", \"apples\", \"oranges\"]\n",
    "df = sqlCtx.createDataFrame([(names[i % 3], items[i % 5]) for i in range(100)], [\"name\", \"item\"])\n",
    "\n",
    "df.show(10)\n",
    "df.stat.crosstab(\"name\", \"item\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  1|  2|  1|\n",
      "|  1|  2|  3|\n",
      "|  3|  6|  3|\n",
      "|  1|  2|  3|\n",
      "|  5| 10|  1|\n",
      "|  1|  2|  3|\n",
      "|  7| 14|  3|\n",
      "|  1|  2|  3|\n",
      "|  9| 18|  1|\n",
      "+---+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(a_freqItems=[1, 99], b_freqItems=[2, 198], c_freqItems=[1, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlCtx.createDataFrame([(1, 2, 3) if i % 2 == 0 else (i, 2 * i, i % 4) for i in range(100)], [\"a\", \"b\", \"c\"])\n",
    "df.show(10)\n",
    "\n",
    "freq = df.stat.freqItems([\"a\", \"b\", \"c\"], 0.4)\n",
    "freq.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     ab_freqItems|\n",
      "+-----------------+\n",
      "|[[99,198], [1,2]]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "freq = df.withColumn('ab', struct('a', 'b')).stat.freqItems(['ab'], 0.4)\n",
    "freq.show()\n",
    "\n",
    "freq.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+\n",
      "|            uniform|  DEGREES(uniform)|     cos^2 + sin^2|\n",
      "+-------------------+------------------+------------------+\n",
      "| 1.2990577122386398| 74.43052425519424|               1.0|\n",
      "| 2.2958798545155346| 131.5442259328496|               1.0|\n",
      "|  2.835954162707124|162.48820441567537|0.9999999999999999|\n",
      "|0.29610844056259905| 16.96576392243732|0.9999999999999999|\n",
      "| 1.2038918656734006| 68.97792289321647|               1.0|\n",
      "| 1.7487432865543826|100.19560977140284|               1.0|\n",
      "| 1.5629166016767624|  89.5485250070077|               1.0|\n",
      "| 0.6520731573472516|37.361039849767565|               1.0|\n",
      "| 3.0055826936438126|172.20720332335196|               1.0|\n",
      "| 2.3328301748181857|133.66132333784805|               1.0|\n",
      "+-------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df = sqlCtx.range(0, 10).withColumn('uniform', rand(seed=10) * 3.14)\n",
    " \n",
    "# you can reference a column or supply the column name\n",
    "df.select('uniform', toDegrees('uniform'), (pow(cos(df['uniform']), 2) + pow(sin(df.uniform), 2)).alias(\"cos^2 + sin^2\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
